{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3288a8d-41a1-4b19-8821-686894424a71",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "- It implements latent dirichlet allocation (a popular topic modeling approach)\n",
    "- The model uses collapsed gibbs sampling (a faster inference model for topic modeling)\n",
    "\n",
    "It operates in two steps.\\\n",
    "\n",
    "*A) Preparing data (integer encoding documents)*  \n",
    "\n",
    "*B) Performing topic modeling on integer encoded documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d213b18-95e0-42d7-a8bd-bda59d703069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\khantr\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f238531-1daf-4776-a790-1e0281e6c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its vanilla implementation of Topic modeling that only uses basic tools:\n",
    "# json - to read from and write to files in json format \n",
    "# numpy - for faster matrix operations \n",
    "# string - to only keep English letters, removing puntuations and other characters\n",
    "# random - to generate random numbers for initializing Markov-chain monte carlo, and \n",
    "#           and during algorithm working to avoid local optima\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e85cc-1cbb-4730-9b2d-7b5fa8e3cdaf",
   "metadata": {},
   "source": [
    "# A) Preparing data (integer encoding documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb420d-bbf4-4c45-9863-8dff51c3848f",
   "metadata": {},
   "source": [
    "1. Read textual data\n",
    "2. Generate integer encoding\n",
    "3. Storing intemediate data\n",
    "\n",
    "**Working with integers (representing words or unique tokens is much faster than the word strings itself)**\n",
    "\n",
    "*At the end, the integers would be reversed back to their respective words*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce624119-ed66-40bc-831b-69bfcf4bd0de",
   "metadata": {},
   "source": [
    "## 1. Reading textual data\n",
    "- Read raw text from .txt file having document per line\n",
    "- Separate into list of documents\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac322afb-36d2-4c35-98c5-856f652d5bf3",
   "metadata": {},
   "source": [
    "1.1 Clean text by removing punctuations and characters othen than English letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f363163-e010-4d32-8c3e-f3d580886247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = [char for char in text if char in string.ascii_lowercase]\n",
    "    return ''.join(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92385f09-70e1-4976-8ed3-8553fd996cdb",
   "metadata": {},
   "source": [
    "1.2 Read data from the file \\\n",
    "1.3 convert to lower case \\\n",
    "1.4 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc2c232-1965-4fd2-a880-56d6cd9e6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as file:\n",
    "    configurations = json.load(file)\n",
    "file = open(configurations[\"text-doc-path\"], 'r')\n",
    "rawdata = file.read()\n",
    "file.close()\n",
    "\n",
    "#split on new line and convert to lower case\n",
    "documents = rawdata.split('\\n')\n",
    "documents = [doc.lower() for doc in documents]\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_documents = []\n",
    "for document in documents:\n",
    "    tokenized_documents.append([token for token in document.split(' ') if len(clean_text(token))>2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9ce6-e97d-4e02-a211-5f599afc3f72",
   "metadata": {},
   "source": [
    "## 2. Generate Integer encoding\n",
    "It preserves both frequency and position related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer ids will be replaced with their original words at the end using stored dictionary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012edbf6-18fe-4044-add9-b5f175baa0e3",
   "metadata": {},
   "source": [
    "2.1 Generate integer encoded documents \\\n",
    "2.2 Generate word-integer index and integer index-word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e324e9f6-b624-4853-9a81-695ff3f1eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d5e66-58d1-44bd-9ef4-34281d186423",
   "metadata": {},
   "source": [
    "### 3. Storing intermediate data\n",
    "The integer encoded documents are stored in files\n",
    "the word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps, each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7122c2b-d37a-4159-9e30-8070dc3af78b",
   "metadata": {},
   "source": [
    "# B) Topic Modeling (LDA)\n",
    "- It identifies the hidden thematic structures within the documents and represent them as latent topics.\n",
    "- Each document is a mixture of all possible topics with varying probabilities\n",
    "- Each topic is a mixture of all vocabulary of the dataset with varying probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a9a10-9280-4df0-9812-77084b1879c1",
   "metadata": {},
   "source": [
    "*Setting random seeds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c350497-fe5e-4216-b04f-e41dd29289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "random.seed(41)  # For Python random\n",
    "np.random.seed(41)  # For NumPy random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17229a3-21b5-4656-8a1c-0733ca137a26",
   "metadata": {},
   "source": [
    "**LDA class**\n",
    "main functions are:\n",
    "1. Markov chain monte carlo initialization (giving the model a random inital state, expecting the model\n",
    "    to converge for higher number of iterations.\n",
    "2. Collapsed gibbs sampling inference: in each iteration \\\n",
    "   2.1 Iterates through all documents, all tokens/words in each document \\\n",
    "   2.2 For for each token computes its most suitable topic, given the current status of the model \\\n",
    "   2.3 Updates new topic if different from current topic, associated estimates update, so does the model state \\\n",
    "3. Estimate document-topic distribution from the final state of the model \n",
    "4. Estimate topic-word distribution (organized in decreasing order of probabilities) from the final state of the model\n",
    "5. Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.01, \n",
    "                 numGSIterations = 1000, numNBurnin = 50, numSampleLag = 20, \n",
    "                 wordsPerTopic = 10):\n",
    "        self._numTopics = configurations[\"numTopics\"]\n",
    "        self._numAlpha = configurations[\"numAlpha\"]\n",
    "        self._numBeta = configurations[\"numBeta\"]\n",
    "        self._numGSIterations = configurations[\"numGSIterations\"]\n",
    "        self._numNBurni = configurations[\"numNBurnin\"]\n",
    "        self._numSampleLag = configurations[\"numSampleLag\"]\n",
    "        self.__wordsPerTopic = configurations[\"wordsPerTopic\"]\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        results = ''\n",
    "        results += \"***Topics per Document***\\n\"\n",
    "        for d in range(self._numDocSize):\n",
    "            results += \"Document \" + str(d) + \":\\n\"\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                results += \"Topic \" + str(t) + \":\" + str(val) + '\\t'\n",
    "            results += '\\n'\n",
    "        print(results)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        results = \"***Words per Topic***\\n\"\n",
    "        \n",
    "        for t in range(self._numTopics):\n",
    "            results += \"\\nTopic \" + str(t) + \":\"\n",
    "            #flag = 0\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                wpt[revdictionary[str(v)]] = float(val)\n",
    "             #   flag += 1\n",
    "             #   if flag == self.__wordsPerTopic:\n",
    "             #       break\n",
    "            results += '\\n'\n",
    "            wpt = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            for item in wpt:\n",
    "                results += str(item)\n",
    "        print(results)\n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99a1e5-6dfc-4611-a3bc-4520acb9d657",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lda = LDA()\n",
    "    lda.getData(configurations[\"integer-encoded-doc-path\"])\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8bd5a3-e674-485d-a1cf-090a843d38ef",
   "metadata": {},
   "source": [
    "## Results\n",
    "- The results are printed on screen and also stored in `data/output-data/` folder\n",
    "- Topics distribution per document\n",
    "- words distribution per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc7c59-b386-40be-b1b5-0b361fdda0a1",
   "metadata": {},
   "source": [
    "*Document topic distribution*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc42807-a4c0-4316-aed8-2870c6358859",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.getTopicsPerDocument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1806691-8039-4174-aaf5-498a9fcd2d46",
   "metadata": {},
   "source": [
    "*Topic word distribution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59102eea-eb42-41c9-bbf9-9a392ee05a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Words per Topic***\n",
      "\n",
      "Topic 0:\n",
      "('the', 0.10956585236325013)('and', 0.03986325013276686)('for', 0.029905735528412112)('economy', 0.029905735528412112)('prices', 0.029905735528412112)('has', 0.019948220924057358)('outlook', 0.019948220924057358)('new', 0.019948220924057358)('menace', 0.019948220924057358)('are', 0.01330987785448752)\n",
      "Topic 1:\n",
      "('oil', 0.08079734784087046)('(reuters)', 0.06379632777966678)('reuters', 0.06379632777966678)('wall', 0.025544032641958515)('band', 0.025544032641958515)('carlyle', 0.025544032641958515)('iraq', 0.025544032641958515)('from', 0.025544032641958515)('main', 0.025544032641958515)('southern', 0.025544032641958515)\n"
     ]
    }
   ],
   "source": [
    "with open(configurations[\"integer-word-dict\"], 'r') as file:\n",
    "    revdictionary = json.load(file)\n",
    "lda.getWordsPerTopic(revdictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d50a05-c34c-45f0-9ec4-a38a65a7651d",
   "metadata": {},
   "source": [
    "*print all details:*\n",
    "- Integer encoded dataset\n",
    "- Final state of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e55f81-a968-4fd2-8f55-7e4dca76564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints everything - for debugging\n",
    "#lda.printall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570046f6-97bb-40c8-8f37-6a03958e81fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
