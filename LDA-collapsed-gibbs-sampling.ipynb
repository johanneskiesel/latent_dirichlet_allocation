{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7272635-565e-4904-bf5b-9e8dba012dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Topics per Document***\n",
      "Document  0 :\n",
      "Topic  0 : 0.5833333333333334 \t\n",
      "Topic  1 : 0.4166666666666667 \t\n",
      "Document  1 :\n",
      "Topic  0 : 0.25 \t\n",
      "Topic  1 : 0.75 \t\n",
      "Document  2 :\n",
      "Topic  0 : 0.25 \t\n",
      "Topic  1 : 0.75 \t\n",
      "Document  3 :\n",
      "Topic  0 : 0.13636363636363635 \t\n",
      "Topic  1 : 0.8636363636363636 \t\n",
      "\n",
      "***Words per Topic***\n",
      "Topic  0 :\n",
      "Vocab  v : 0.5042735042735044 \t\n",
      "Vocab  v : 0.24786324786324787 \t\n",
      "Vocab  v : 0.24786324786324787 \t\n",
      "Topic  1 :\n",
      "Vocab  v : 0.10344827586206895 \t\n",
      "Vocab  v : 0.44827586206896547 \t\n",
      "Vocab  v : 0.44827586206896547 \t\n"
     ]
    }
   ],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.1, \n",
    "                 numGSIterations = 2000, numNBurnin = 50, numSampleLag = 20):\n",
    "        self._numTopics = numTopics\n",
    "        self._numAlpha = numAlpha\n",
    "        self._numBeta = numBeta\n",
    "        self._numGSIterations = numGSIterations\n",
    "        self._numNBurni = numNBurnin\n",
    "        self._numSampleLag = numSampleLag\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            \n",
    "                            \n",
    "                    #update as per new topic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[newTopic] -= 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        print(\"\\n***Topics per Document***\")\n",
    "        for d in range(self._numDocSize):\n",
    "            print(\"Document \", d, \":\")\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                print(\"Topic \", t, \":\", val, '\\t')\n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self):\n",
    "        print(\"\\n***Words per Topic***\")\n",
    "        for t in range(self._numTopics):\n",
    "            print(\"Topic \", t, \":\")\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                print(\"Vocab \", \"v\", \":\", val, '\\t')\n",
    "            \n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lda = LDA(numGSIterations = 1)\n",
    "    lda.getData(\"data/inputdata.txt\")\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()\n",
    "    lda.getTopicsPerDocument()\n",
    "    lda.getWordsPerTopic()\n",
    "    #lda.printall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a9dc5-8d49-4f12-901e-feba48947036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
