{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3288a8d-41a1-4b19-8821-686894424a71",
   "metadata": {},
   "source": [
    "# Topic modeling (LDA)\n",
    "- It implements latent dirichlet allocation (a popular topic modeling approach)\n",
    "- The model uses collapsed gibbs sampling (a faster inference model for topic modeling)\n",
    "\n",
    "It operates in two steps.\n",
    "\n",
    "*A) Preparing data (integer encoding documents)*  \n",
    "\n",
    "*B) Performing topic modeling on integer encoded documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0f238531-1daf-4776-a790-1e0281e6c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its vanilla implementation of Topic modeling that only uses basic tools:\n",
    "# json - to read from and write to files in json format \n",
    "# numpy - for faster matrix operations \n",
    "# pandas - to read csv data\n",
    "# string - to only keep English letters, removing puntuations and other characters\n",
    "# random - to generate random numbers for initializing Markov-chain monte carlo, and \n",
    "#           and during algorithm working to avoid local optima\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e85cc-1cbb-4730-9b2d-7b5fa8e3cdaf",
   "metadata": {},
   "source": [
    "# A) Preparing data (integer encoding documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb420d-bbf4-4c45-9863-8dff51c3848f",
   "metadata": {},
   "source": [
    "1. Read textual data\n",
    "2. Generate integer encoding\n",
    "3. Storing intemediate data\n",
    "\n",
    "**Working with integers (representing words or unique tokens is much faster than the word strings itself)**\n",
    "\n",
    "*At the end, the integers would be reversed back to their respective words*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce624119-ed66-40bc-831b-69bfcf4bd0de",
   "metadata": {},
   "source": [
    "## 1. Reading textual data\n",
    "- Read raw text from .txt file having document per line\n",
    "- Separate into list of documents\n",
    "- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac322afb-36d2-4c35-98c5-856f652d5bf3",
   "metadata": {},
   "source": [
    "1.1 Clean text by removing punctuations and characters othen than English letters \\\n",
    "1.2 Convert to lower case \\\n",
    "1.3 Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3f363163-e010-4d32-8c3e-f3d580886247",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    # cleaning documents by removing unwanted characters\n",
    "    clean_text = \"\".join([char for char in text if char in string.ascii_lowercase])\n",
    "    # cleaning documents by stopwords\n",
    "    clean_text = [word for word in text.split(\" \") if word not in en_stopwords and len(word) > 2]\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92385f09-70e1-4976-8ed3-8553fd996cdb",
   "metadata": {},
   "source": [
    "1.4 Read data from the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4cc2c232-1965-4fd2-a880-56d6cd9e6ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read input data: titles of BBC articles available on the following link\n",
    "# https://github.com/vahadruya/Capstone-Project-Unsupervised-ML-Topic-Modelling/blob/main/Input_Data/input.csv\n",
    "\n",
    "with open('config.json', 'r') as file:\n",
    "    configurations = json.load(file)\n",
    "df = pd.read_csv(configurations[\"text-doc-path\"])\n",
    "rawdata = df[\"Title\"].tolist()\n",
    "\n",
    "\n",
    "# Tokenize sentences into words\n",
    "tokenized_documents = []\n",
    "for document in rawdata[:100]: # Considering only first 100 titles for the sake of demonstration\n",
    "    document = clean_text(document)\n",
    "    if len(document) > 2:\n",
    "        tokenized_documents.append(document)\n",
    "len(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc667374-bf14-4907-8230-354aa6235ec2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be5529-ce14-47b5-834e-30fee7263b0f",
   "metadata": {},
   "source": [
    "`config.json`\n",
    "\n",
    "The method provides the following configuration options to alter the behavior of the method.\n",
    "\n",
    "**numTopics:** Number of topics to extract from the dataset. The default value is 3. However, it generally depends on the nature of the data. Keeping the number of topics too few can only have the topics focused on broader concepts and cannot identify the specific topics. While keeping the number of topics too high results in noisy (incoherent) topics. Therefore, a suitable number of topics depends on the data and the type of analysis required. The default value here is 3.\n",
    "\n",
    "**numAlpha**: This hyperparameter helps in deciding the probabilities of topics in a document. A higher value (above 1) will have too many topics with similar probabilities i.e., when the intended purpose is to get more topics per document. However, keeping the value too low will have only few promiment topics with very high probabilities as compared to others. A lower value (below 1) is used when only fewer topics are needed per document. A lower numAlpha value pushes higher probabilities higher and lower probabilities further lower. While a higher numAlpha value introduces a high bias due to which all probabilities converges to similar values. In this method we are using numAlpha value 1.\n",
    "\n",
    "**numBeta**: This hyperparameter helps in deciding the probabilities of words in a topic. a higher value (above 1) will have too many words with similar probabilities i.e., when the intended purpose is to have more words representing a topic. However, keeping the value too low will have only few most prominent words with very high probabilities as compared to others. Generally, considering the size of vocabulary in a dataset, this value is kept smaller to determine only the most relevant words. In this method we are using numBeta value 0.01.\n",
    "\n",
    "**numGSIterations**: Its the number of iterations of the inference technique (collapsed Gibbs sampling). Due to random initialization, there are more words switching their topics in the earlier iterations that keeps dropping in the coming iterations i.e., approaching the equilibrium state. Keeping the number of numGSIterations higher ensures that the words have settled down in their respective topics. Alternately, the difference between two consecutive iterations is also used to avoid unnecessary iterations when the words have already settled. In this method we only use the numGSIterations with value 1000.\n",
    "\n",
    "**wordsPerTopic**: The number of top words to represent a topic. In this method we are using 10 most prominent words for each topic. Due to polysemy, it is possible that a word exist in different topics with different neighboring words highlighting its context. \n",
    "\n",
    "**text-doc-path**: path of input (raw text) file\n",
    "**integer-encoded-doc-path**: path of integer encoded file. It is the intermediate file that topic modeling use. \n",
    "**integer-word-dict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea9ce6-e97d-4e02-a211-5f599afc3f72",
   "metadata": {},
   "source": [
    "## 2. Generate Integer encoding\n",
    "It preserves both frequency and position related information. The process involves assigning each unique token a dedicated integer id, preserving it in a dictionary for later retrieval, while rewriting documents by replacing with with their integer ids.\n",
    "\n",
    "It makes the operations a lot faster as numbers are much faster to read/store and compare as compared to strings. \n",
    "\n",
    "The integer ids will be replaced with their original words at the end using stored dictionary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012edbf6-18fe-4044-add9-b5f175baa0e3",
   "metadata": {},
   "source": [
    "2.1 Generate integer encoded documents \\\n",
    "2.2 Generate word-integer index and integer index-word dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e324e9f6-b624-4853-9a81-695ff3f1eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique tokens and assign integers\n",
    "dictionary = {}\n",
    "revdictionary = {}\n",
    "index = 0\n",
    "\n",
    "#tokenized_documents = [[word for word in doc if word not in esw] for doc in tokenized_documents]\n",
    "\n",
    "for doc in tokenized_documents:\n",
    "    for word in doc:\n",
    "        if word not in dictionary.keys():\n",
    "            dictionary[word] = index\n",
    "            revdictionary[index] = word\n",
    "            index += 1\n",
    "\n",
    "# Replace words in sentences with their corresponding integers\n",
    "encoded_documents = [[dictionary[word] for word in doc] for doc in tokenized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05d5e66-58d1-44bd-9ef4-34281d186423",
   "metadata": {},
   "source": [
    "### 3. Storing intermediate data\n",
    "The integer encoded documents are stored in files\n",
    "the word-to-id and id-to-word dictionaries are also stored\n",
    "\n",
    "*It will help to avoid these steps, each time topic modeling is performed under different settings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "toStr = ''\n",
    "for endoc in encoded_documents:\n",
    "    toStr = toStr + '\\t'.join(str(item) for item in endoc)\n",
    "    toStr = toStr + '\\n'\n",
    "toStr = toStr[:-2]\n",
    "file = open('data/integer-encoded-data.txt', 'w')\n",
    "file.write(toStr)\n",
    "file.close()\n",
    "\n",
    "#write dictionary to file\n",
    "file = open('data/dictionary.json', 'w')\n",
    "file.write(json.dumps(dictionary))\n",
    "file.close()\n",
    "file = open('data/revdictionary.json', 'w')\n",
    "file.write(json.dumps(revdictionary))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7122c2b-d37a-4159-9e30-8070dc3af78b",
   "metadata": {},
   "source": [
    "# B) Topic Modeling (LDA)\n",
    "- It identifies the hidden thematic structures within the documents and represent them as latent topics.\n",
    "- Each document is a mixture of all possible topics with varying probabilities\n",
    "- Each topic is a mixture of all vocabulary of the dataset with varying probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8a9a10-9280-4df0-9812-77084b1879c1",
   "metadata": {},
   "source": [
    "*Setting random seeds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0c350497-fe5e-4216-b04f-e41dd29289bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducible results\n",
    "random.seed(41)  # For Python random\n",
    "np.random.seed(41)  # For NumPy random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17229a3-21b5-4656-8a1c-0733ca137a26",
   "metadata": {},
   "source": [
    "**LDA class**\n",
    "main functions are:\n",
    "1. Markov chain monte carlo initialization (giving the model a random inital state, expecting the model\n",
    "    to converge for higher number of iterations.\n",
    "2. Collapsed gibbs sampling inference: in each iteration \\\n",
    "   2.1 Iterates through all documents, all tokens/words in each document \\\n",
    "   2.2 For for each token computes its most suitable topic, given the current status of the model \\\n",
    "   2.3 Updates new topic if different from current topic, associated estimates update, so does the model state \\\n",
    "3. Estimate document-topic distribution from the final state of the model \n",
    "4. Estimate topic-word distribution (organized in decreasing order of probabilities) from the final state of the model\n",
    "5. Other utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.01, \n",
    "                 numGSIterations = 1000, numNBurnin = 50, numSampleLag = 20, \n",
    "                 wordsPerTopic = 10):\n",
    "        self._numTopics = configurations[\"numTopics\"]\n",
    "        self._numAlpha = configurations[\"numAlpha\"]\n",
    "        self._numBeta = configurations[\"numBeta\"]\n",
    "        self._numGSIterations = configurations[\"numGSIterations\"]\n",
    "        self._numNBurni = configurations[\"numNBurnin\"]\n",
    "        self._numSampleLag = configurations[\"numSampleLag\"]\n",
    "        self.__wordsPerTopic = configurations[\"wordsPerTopic\"]\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        results = ''\n",
    "        results += \"***Topics per Document***\\n\"\n",
    "        for d in range(self._numDocSize):\n",
    "            results += \"Document \" + str(d) + \":\\n\"\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                results += \"Topic \" + str(t) + \":\" + str(val) + '\\t'\n",
    "            results += '\\n'\n",
    "        print(results)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        results = \"***Words per Topic***\\n\"\n",
    "        \n",
    "        for t in range(self._numTopics):\n",
    "            results += \"\\nTopic \" + str(t) + \":\"\n",
    "            #flag = 0\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                wpt[revdictionary[str(v)]] = float(val)\n",
    "             #   flag += 1\n",
    "             #   if flag == self.__wordsPerTopic:\n",
    "             #       break\n",
    "            results += '\\n'\n",
    "            wpt = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            for item in wpt:\n",
    "                results += str(item)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "        print(results)\n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d99a1e5-6dfc-4611-a3bc-4520acb9d657",
   "metadata": {},
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    lda = LDA()\n",
    "    lda.getData(configurations[\"integer-encoded-doc-path\"])\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8bd5a3-e674-485d-a1cf-090a843d38ef",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Topic modeling has two important results\n",
    "1. **Latent topics** identified in the corpus. Each topic is represented by top most presentable words for that topic. Its similar to clustering in the sense that the words are grouped as topics and labeled unintuitively as topic 0, topic 1 etc. However, unlike clustering, the words have probabilities of relevance to the other words of the topic. Using these probabilities, only the top few words (10 or 20) are used to represent a topic. Therefore, it is also called *word topic distribution*. \n",
    "\n",
    "2. **Topics in documents** are the probabilities of topics within each document. A general conception is that a document is not about entirely about a single topic and instead has different percentages of multiple topics. The topics in documents provide the probabilities of each topic in each document.\n",
    "\n",
    "To observe the output generated by this method closely, please goto `data/output-data/` folder having `word-topic-distribution.txt` and `document-topic-distribution.txt` \n",
    "\n",
    "**words distribution per topic** \\\n",
    "The three latent topics determind from this dataset are labeled as Topic 0, Topic 1, and Topic 2. \\\n",
    "*Topic 0:* Represents profit, production and growth in economy \\\n",
    "*Topic 1:* Represents trade and and deals of fuel and BMW mentioning Germany and France \\\n",
    "*Topic 2:* Represents jobs, Yukos firm, India and Japan\n",
    "\n",
    "These three topics gives a general idea of the topics covered in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1806691-8039-4174-aaf5-498a9fcd2d46",
   "metadata": {},
   "source": [
    "*Topic word distribution*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "59102eea-eb42-41c9-bbf9-9a392ee05a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Words per Topic***\n",
      "\n",
      "Topic 0:\n",
      "('profits', 0.028118645256293383)('profit', 0.028118645256293383)('firm', 0.028118645256293383)('Japan', 0.028118645256293383)('Japanese', 0.021106514269686554)('2004', 0.014094383283079725)('prices', 0.014094383283079725)('Parmalat', 0.014094383283079725)('recession', 0.014094383283079725)('wine', 0.014094383283079725)\n",
      "Topic 1:\n",
      "('hits', 0.03138901071361443)('jobs', 0.025123739114090594)('growth', 0.025123739114090594)('India', 0.018858467514566754)('new', 0.018858467514566754)('oil', 0.018858467514566754)('trade', 0.012593195915042914)('takeover', 0.012593195915042914)('lifts', 0.012593195915042914)('production', 0.012593195915042914)\n",
      "Topic 2:\n",
      "('economy', 0.0381320982171182)('deal', 0.0381320982171182)('fuel', 0.0317873231393947)('Yukos', 0.02544254806167121)('gets', 0.02544254806167121)('German', 0.019097772983947717)('sales', 0.019097772983947717)('BMW', 0.019097772983947717)('rise', 0.012752997906224223)('$280bn', 0.012752997906224223)\n"
     ]
    }
   ],
   "source": [
    "with open(configurations[\"integer-word-dict\"], 'r') as file:\n",
    "    revdictionary = json.load(file)\n",
    "lda.getWordsPerTopic(revdictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71cf8f2-ae9d-458f-bd03-742ec71a16ce",
   "metadata": {},
   "source": [
    "**Topic distribution per document** \\\n",
    "Each document talks about the topics identified to different extent. For example, document 0 can be 45% topic 0, 45% topic 1 and 10% topic 2. \n",
    "\n",
    "Therefore, it is important to know which documents are dominated by which topics, so that if a reader is interested in knowing particularly about topic 1 they can only read the documents where topic 1 is the major topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4dc42807-a4c0-4316-aed8-2870c6358859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Topics per Document***\n",
      "Document 0:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 1:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 2:\n",
      "Topic 0:0.375\tTopic 1:0.125\tTopic 2:0.5\t\n",
      "Document 3:\n",
      "Topic 0:0.5\tTopic 1:0.125\tTopic 2:0.375\t\n",
      "Document 4:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.42857142857142855\tTopic 2:0.2857142857142857\t\n",
      "Document 5:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 6:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 7:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 8:\n",
      "Topic 0:0.25\tTopic 1:0.375\tTopic 2:0.375\t\n",
      "Document 9:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.42857142857142855\tTopic 2:0.14285714285714285\t\n",
      "Document 10:\n",
      "Topic 0:0.4444444444444444\tTopic 1:0.2222222222222222\tTopic 2:0.3333333333333333\t\n",
      "Document 11:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.2857142857142857\tTopic 2:0.5714285714285714\t\n",
      "Document 12:\n",
      "Topic 0:0.5\tTopic 1:0.125\tTopic 2:0.375\t\n",
      "Document 13:\n",
      "Topic 0:0.2222222222222222\tTopic 1:0.1111111111111111\tTopic 2:0.6666666666666666\t\n",
      "Document 14:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 15:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.42857142857142855\tTopic 2:0.42857142857142855\t\n",
      "Document 16:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 17:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 18:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.42857142857142855\tTopic 2:0.42857142857142855\t\n",
      "Document 19:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 20:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 21:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 22:\n",
      "Topic 0:0.4444444444444444\tTopic 1:0.2222222222222222\tTopic 2:0.3333333333333333\t\n",
      "Document 23:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 24:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 25:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 26:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 27:\n",
      "Topic 0:0.5\tTopic 1:0.125\tTopic 2:0.375\t\n",
      "Document 28:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 29:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 30:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 31:\n",
      "Topic 0:0.75\tTopic 1:0.125\tTopic 2:0.125\t\n",
      "Document 32:\n",
      "Topic 0:0.25\tTopic 1:0.375\tTopic 2:0.375\t\n",
      "Document 33:\n",
      "Topic 0:0.2222222222222222\tTopic 1:0.2222222222222222\tTopic 2:0.5555555555555556\t\n",
      "Document 34:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 35:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 36:\n",
      "Topic 0:0.625\tTopic 1:0.125\tTopic 2:0.25\t\n",
      "Document 37:\n",
      "Topic 0:0.125\tTopic 1:0.125\tTopic 2:0.75\t\n",
      "Document 38:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 39:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 40:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 41:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 42:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 43:\n",
      "Topic 0:0.125\tTopic 1:0.75\tTopic 2:0.125\t\n",
      "Document 44:\n",
      "Topic 0:0.75\tTopic 1:0.125\tTopic 2:0.125\t\n",
      "Document 45:\n",
      "Topic 0:0.125\tTopic 1:0.125\tTopic 2:0.75\t\n",
      "Document 46:\n",
      "Topic 0:0.375\tTopic 1:0.5\tTopic 2:0.125\t\n",
      "Document 47:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 48:\n",
      "Topic 0:0.625\tTopic 1:0.25\tTopic 2:0.125\t\n",
      "Document 49:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 50:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 51:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "Document 52:\n",
      "Topic 0:0.1111111111111111\tTopic 1:0.3333333333333333\tTopic 2:0.5555555555555556\t\n",
      "Document 53:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.2857142857142857\tTopic 2:0.14285714285714285\t\n",
      "Document 54:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 55:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.2857142857142857\tTopic 2:0.42857142857142855\t\n",
      "Document 56:\n",
      "Topic 0:0.75\tTopic 1:0.125\tTopic 2:0.125\t\n",
      "Document 57:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 58:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 59:\n",
      "Topic 0:0.75\tTopic 1:0.125\tTopic 2:0.125\t\n",
      "Document 60:\n",
      "Topic 0:0.3333333333333333\tTopic 1:0.3333333333333333\tTopic 2:0.3333333333333333\t\n",
      "Document 61:\n",
      "Topic 0:0.5\tTopic 1:0.16666666666666666\tTopic 2:0.3333333333333333\t\n",
      "Document 62:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.14285714285714285\tTopic 2:0.42857142857142855\t\n",
      "Document 63:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 64:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 65:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.14285714285714285\tTopic 2:0.2857142857142857\t\n",
      "Document 66:\n",
      "Topic 0:0.125\tTopic 1:0.125\tTopic 2:0.75\t\n",
      "Document 67:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.42857142857142855\tTopic 2:0.42857142857142855\t\n",
      "Document 68:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 69:\n",
      "Topic 0:0.3333333333333333\tTopic 1:0.3333333333333333\tTopic 2:0.3333333333333333\t\n",
      "Document 70:\n",
      "Topic 0:0.375\tTopic 1:0.25\tTopic 2:0.375\t\n",
      "Document 71:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 72:\n",
      "Topic 0:0.16666666666666666\tTopic 1:0.5\tTopic 2:0.3333333333333333\t\n",
      "Document 73:\n",
      "Topic 0:0.25\tTopic 1:0.625\tTopic 2:0.125\t\n",
      "Document 74:\n",
      "Topic 0:0.2222222222222222\tTopic 1:0.1111111111111111\tTopic 2:0.6666666666666666\t\n",
      "Document 75:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.42857142857142855\tTopic 2:0.14285714285714285\t\n",
      "Document 76:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 77:\n",
      "Topic 0:0.375\tTopic 1:0.375\tTopic 2:0.25\t\n",
      "Document 78:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.42857142857142855\tTopic 2:0.2857142857142857\t\n",
      "Document 79:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.2857142857142857\tTopic 2:0.2857142857142857\t\n",
      "Document 80:\n",
      "Topic 0:0.625\tTopic 1:0.25\tTopic 2:0.125\t\n",
      "Document 81:\n",
      "Topic 0:0.25\tTopic 1:0.5\tTopic 2:0.25\t\n",
      "Document 82:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 83:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.42857142857142855\tTopic 2:0.14285714285714285\t\n",
      "Document 84:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.5714285714285714\tTopic 2:0.14285714285714285\t\n",
      "Document 85:\n",
      "Topic 0:0.25\tTopic 1:0.375\tTopic 2:0.375\t\n",
      "Document 86:\n",
      "Topic 0:0.125\tTopic 1:0.5\tTopic 2:0.375\t\n",
      "Document 87:\n",
      "Topic 0:0.5\tTopic 1:0.16666666666666666\tTopic 2:0.3333333333333333\t\n",
      "Document 88:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 89:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.7142857142857143\tTopic 2:0.14285714285714285\t\n",
      "Document 90:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.14285714285714285\tTopic 2:0.14285714285714285\t\n",
      "Document 91:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.5714285714285714\tTopic 2:0.2857142857142857\t\n",
      "Document 92:\n",
      "Topic 0:0.16666666666666666\tTopic 1:0.16666666666666666\tTopic 2:0.6666666666666666\t\n",
      "Document 93:\n",
      "Topic 0:0.125\tTopic 1:0.125\tTopic 2:0.75\t\n",
      "Document 94:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.14285714285714285\tTopic 2:0.7142857142857143\t\n",
      "Document 95:\n",
      "Topic 0:0.125\tTopic 1:0.25\tTopic 2:0.625\t\n",
      "Document 96:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.42857142857142855\tTopic 2:0.14285714285714285\t\n",
      "Document 97:\n",
      "Topic 0:0.2857142857142857\tTopic 1:0.14285714285714285\tTopic 2:0.5714285714285714\t\n",
      "Document 98:\n",
      "Topic 0:0.25\tTopic 1:0.125\tTopic 2:0.625\t\n",
      "Document 99:\n",
      "Topic 0:0.125\tTopic 1:0.375\tTopic 2:0.5\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lda.getTopicsPerDocument()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc7c59-b386-40be-b1b5-0b361fdda0a1",
   "metadata": {},
   "source": [
    "*Document topic distribution*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d50a05-c34c-45f0-9ec4-a38a65a7651d",
   "metadata": {},
   "source": [
    "*print all details:*\n",
    "- Integer encoded dataset\n",
    "- Final state of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "31e55f81-a968-4fd2-8f55-7e4dca76564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints everything - for debugging\n",
    "#lda.printall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570046f6-97bb-40c8-8f37-6a03958e81fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0f495-c11d-458d-89b4-421ab7109ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
