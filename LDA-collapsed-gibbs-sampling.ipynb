{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7272635-565e-4904-bf5b-9e8dba012dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics:  2\n",
      "dataset:  [[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 4], [16, 17, 18, 19, 20], [1, 21, 22, 23, 24, 25], [26, 27, 28, 29, 30, 31], [32, 33, 34, 35, 36, 37], [38, 39, 40, 41, 42, 39], [43, 44, 45, 46, 47, 48, 49, 50], [51, 52, 53, 54, 55], [56, 57, 58, 59, 60], [61, 44, 62, 63, 64], [65, 66, 67, 58, 68], [69, 70, 71, 72, 73, 74], [75, 76, 77, 78, 79], [80, 81, 82, 73, 83], [84, 85, 66, 86, 87], [88, 89, 90, 91, 92], [93, 79, 94, 95, 9]]\n",
      "dataset size:  20\n",
      "vocab:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]\n",
      "vocab size:  96\n",
      "ndt:  [[3 1]\n",
      " [3 2]\n",
      " [4 0]\n",
      " [4 0]\n",
      " [5 0]\n",
      " [0 6]\n",
      " [0 6]\n",
      " [1 5]\n",
      " [1 5]\n",
      " [5 3]\n",
      " [3 2]\n",
      " [4 1]\n",
      " [5 0]\n",
      " [0 5]\n",
      " [6 0]\n",
      " [0 5]\n",
      " [5 0]\n",
      " [4 1]\n",
      " [2 3]\n",
      " [2 3]]\n",
      "ndsum:  [4 5 4 4 5 6 6 6 6 8 5 5 5 5 6 5 5 5 5 5]\n",
      "ntw:  [[1 1 1 0 1 1 1 1 0 2 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 1 1 2 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
      "  1 2 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 0 0]\n",
      " [0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      "  1 1 1 2 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 2 0 0 0 0 0 0 1 2 1 1 0 0 0\n",
      "  0 0 0 1 1 1 1 2 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1]]\n",
      "ntsum:  [57 48]\n",
      "z:  [[0, 0, 0, 1], [1, 0, 0, 0, 1], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1], [1, 1, 1, 1, 0, 1], [0, 0, 1, 0, 0, 1, 1, 0], [0, 0, 0, 1, 1], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, 1, 0, 1], [0, 1, 1, 1, 0]]\n",
      "***Topics per Document***\n",
      "Document 0:\n",
      "Topic 0:0.6666666666666666\tTopic 1:0.3333333333333333\t\n",
      "Document 1:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.42857142857142855\t\n",
      "Document 2:\n",
      "Topic 0:0.8333333333333334\tTopic 1:0.16666666666666666\t\n",
      "Document 3:\n",
      "Topic 0:0.8333333333333334\tTopic 1:0.16666666666666666\t\n",
      "Document 4:\n",
      "Topic 0:0.8571428571428571\tTopic 1:0.14285714285714285\t\n",
      "Document 5:\n",
      "Topic 0:0.125\tTopic 1:0.875\t\n",
      "Document 6:\n",
      "Topic 0:0.125\tTopic 1:0.875\t\n",
      "Document 7:\n",
      "Topic 0:0.25\tTopic 1:0.75\t\n",
      "Document 8:\n",
      "Topic 0:0.25\tTopic 1:0.75\t\n",
      "Document 9:\n",
      "Topic 0:0.6\tTopic 1:0.4\t\n",
      "Document 10:\n",
      "Topic 0:0.5714285714285714\tTopic 1:0.42857142857142855\t\n",
      "Document 11:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.2857142857142857\t\n",
      "Document 12:\n",
      "Topic 0:0.8571428571428571\tTopic 1:0.14285714285714285\t\n",
      "Document 13:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.8571428571428571\t\n",
      "Document 14:\n",
      "Topic 0:0.875\tTopic 1:0.125\t\n",
      "Document 15:\n",
      "Topic 0:0.14285714285714285\tTopic 1:0.8571428571428571\t\n",
      "Document 16:\n",
      "Topic 0:0.8571428571428571\tTopic 1:0.14285714285714285\t\n",
      "Document 17:\n",
      "Topic 0:0.7142857142857143\tTopic 1:0.2857142857142857\t\n",
      "Document 18:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.5714285714285714\t\n",
      "Document 19:\n",
      "Topic 0:0.42857142857142855\tTopic 1:0.5714285714285714\t\n",
      "\n",
      "***Words per Topic***\n",
      "\n",
      "Topic 0:\n",
      "('enjoy', 0.01651651651651652)('cooking', 0.01651651651651652)('pasta', 0.01651651651651652)('baking', 0.01651651651651652)('dinner', 0.0015015015015015017)\n",
      "Topic 1:\n",
      "('cooking', 0.019097222222222224)('dinner', 0.019097222222222224)('baking', 0.019097222222222224)('enjoy', 0.0017361111111111112)('pasta', 0.0017361111111111112)\n"
     ]
    }
   ],
   "source": [
    "random.seed(41)  # For Python random\n",
    "np.random.seed(41)  # For NumPy random\n",
    "\n",
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.1, \n",
    "                 numGSIterations = 2000, numNBurnin = 50, numSampleLag = 20, wordsPerTopic=5):\n",
    "        self._numTopics = numTopics\n",
    "        self._numAlpha = numAlpha\n",
    "        self._numBeta = numBeta\n",
    "        self._numGSIterations = numGSIterations\n",
    "        self._numNBurni = numNBurnin\n",
    "        self._numSampleLag = numSampleLag\n",
    "        self.__wordsPerTopic = wordsPerTopic\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            break\n",
    "                    #update as per new topic\n",
    "                    self.__arrZ[d][ind] = newTopic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] += 1\n",
    "                    self.__arrNTSum[newTopic] += 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        results = ''\n",
    "        results += \"***Topics per Document***\\n\"\n",
    "        for d in range(self._numDocSize):\n",
    "            results += \"Document \" + str(d) + \":\\n\"\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                results += \"Topic \" + str(t) + \":\" + str(val) + '\\t'\n",
    "            results += '\\n'\n",
    "        print(results)\n",
    "        file = open('data/output-data/document-topic-distribution.txt', 'w')\n",
    "        file.write(results)\n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        results = \"***Words per Topic***\\n\"\n",
    "        \n",
    "        for t in range(self._numTopics):\n",
    "            results += \"\\nTopic \" + str(t) + \":\"\n",
    "            flag = 0\n",
    "            wpt = {}\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                wpt[revdictionary[str(v)]] = float(val)\n",
    "                flag += 1\n",
    "                if flag == self.__wordsPerTopic:\n",
    "                    break\n",
    "            results += '\\n'\n",
    "            wpt = sorted(wpt.items(), key=lambda x: x[1], reverse=True)[:self.__wordsPerTopic]\n",
    "            for item in wpt:\n",
    "                results += str(item)\n",
    "        print(results)\n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lda = LDA(numGSIterations = 100)\n",
    "    lda.getData(\"data/integer-encoded-data.txt\")\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()\n",
    "    lda.printall()\n",
    "    lda.getTopicsPerDocument()\n",
    "    with open('data/revdictionary.json', 'r') as file:\n",
    "            revdictionary = json.load(file)\n",
    "    lda.getWordsPerTopic(revdictionary)\n",
    "    #lda.printall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d28c4d5-b279-495d-a347-7e365cf35a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
