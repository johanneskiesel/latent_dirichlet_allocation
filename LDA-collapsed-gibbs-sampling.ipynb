{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e3caf-ef52-4ac3-9577-7a77795b0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed953da4-5dd5-449f-838f-a74389615e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7272635-565e-4904-bf5b-9e8dba012dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77b49ce-7684-40ac-9f15-1184d6887b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***Topics per Document***\n",
      "Document  0 :\n",
      "Topic  0 : 0.043478260869565216 \t\n",
      "Topic  1 : 0.9565217391304348 \t\n",
      "Document  1 :\n",
      "Topic  0 : 0.05263157894736842 \t\n",
      "Topic  1 : 0.9473684210526315 \t\n",
      "Document  2 :\n",
      "Topic  0 : 0.05263157894736842 \t\n",
      "Topic  1 : 0.9473684210526315 \t\n",
      "Document  3 :\n",
      "Topic  0 : 0.15789473684210525 \t\n",
      "Topic  1 : 0.8421052631578947 \t\n",
      "Document  4 :\n",
      "Topic  0 : 0.1282051282051282 \t\n",
      "Topic  1 : 0.8717948717948718 \t\n",
      "Document  5 :\n",
      "Topic  0 : 0.25 \t\n",
      "Topic  1 : 0.75 \t\n",
      "Document  6 :\n",
      "Topic  0 : 0.2702702702702703 \t\n",
      "Topic  1 : 0.7297297297297297 \t\n",
      "Document  7 :\n",
      "Topic  0 : 0.2926829268292683 \t\n",
      "Topic  1 : 0.7073170731707317 \t\n",
      "Document  8 :\n",
      "Topic  0 : 0.7 \t\n",
      "Topic  1 : 0.3 \t\n",
      "Document  9 :\n",
      "Topic  0 : 0.16 \t\n",
      "Topic  1 : 0.84 \t\n",
      "Document  10 :\n",
      "Topic  0 : 0.16666666666666666 \t\n",
      "Topic  1 : 0.8333333333333334 \t\n",
      "Document  11 :\n",
      "Topic  0 : 0.23404255319148937 \t\n",
      "Topic  1 : 0.7659574468085106 \t\n",
      "Document  12 :\n",
      "Topic  0 : 0.20689655172413793 \t\n",
      "Topic  1 : 0.7931034482758621 \t\n",
      "Document  13 :\n",
      "Topic  0 : 0.23255813953488372 \t\n",
      "Topic  1 : 0.7674418604651163 \t\n",
      "Document  14 :\n",
      "Topic  0 : 0.6458333333333334 \t\n",
      "Topic  1 : 0.3541666666666667 \t\n",
      "Document  15 :\n",
      "Topic  0 : 0.7333333333333333 \t\n",
      "Topic  1 : 0.26666666666666666 \t\n",
      "Document  16 :\n",
      "Topic  0 : 0.5555555555555556 \t\n",
      "Topic  1 : 0.4444444444444444 \t\n",
      "Document  17 :\n",
      "Topic  0 : 0.5306122448979592 \t\n",
      "Topic  1 : 0.46938775510204084 \t\n",
      "Document  18 :\n",
      "Topic  0 : 0.3333333333333333 \t\n",
      "Topic  1 : 0.6666666666666666 \t\n",
      "\n",
      "***Words per Topic***\n",
      "Topic  0 :\n",
      "Vocab  0 - wall : -0.000552791597567717 \t\n",
      "Vocab  1 - st. : -0.000552791597567717 \t\n",
      "Vocab  2 - bears : -0.000552791597567717 \t\n",
      "Vocab  3 - claw : -0.000552791597567717 \t\n",
      "Vocab  4 - back : 0.004975124378109453 \t\n",
      "Vocab  5 - into : -0.000552791597567717 \t\n",
      "Vocab  6 - the : 0.027086788280818133 \t\n",
      "Vocab  7 - black : -0.000552791597567717 \t\n",
      "Vocab  8 - (reuters) : -0.000552791597567717 \t\n",
      "Vocab  9 - reuters : 0.016030956329463792 \t\n",
      "Vocab  10 - - : 0.032614704256495305 \t\n",
      "Vocab  11 - short-sellers, : -0.000552791597567717 \t\n",
      "Vocab  12 - street's : -0.000552791597567717 \t\n",
      "Vocab  13 - dwindling\\band : -0.000552791597567717 \t\n",
      "Vocab  14 - of : 0.004975124378109453 \t\n",
      "Vocab  15 - ultra-cynics, : -0.000552791597567717 \t\n",
      "Vocab  16 - are : -0.000552791597567717 \t\n",
      "Vocab  17 - seeing : -0.000552791597567717 \t\n",
      "Vocab  18 - green : -0.000552791597567717 \t\n",
      "Vocab  19 - again. : -0.000552791597567717 \t\n",
      "Topic  1 :\n",
      "Vocab  0 - wall : 0.008926527809567406 \t\n",
      "Vocab  1 - st. : 0.00434882124055848 \t\n",
      "Vocab  2 - bears : 0.00434882124055848 \t\n",
      "Vocab  3 - claw : 0.00434882124055848 \t\n",
      "Vocab  4 - back : 0.00434882124055848 \t\n",
      "Vocab  5 - into : 0.00434882124055848 \t\n",
      "Vocab  6 - the : 0.06614785992217899 \t\n",
      "Vocab  7 - black : 0.00434882124055848 \t\n",
      "Vocab  8 - (reuters) : 0.02494850080109865 \t\n",
      "Vocab  9 - reuters : 0.00434882124055848 \t\n",
      "Vocab  10 - - : 0.020370794232089724 \t\n",
      "Vocab  11 - short-sellers, : 0.00434882124055848 \t\n",
      "Vocab  12 - street's : 0.00434882124055848 \t\n",
      "Vocab  13 - dwindling\\band : 0.002059967956054017 \t\n",
      "Vocab  14 - of : 0.020370794232089724 \t\n",
      "Vocab  15 - ultra-cynics, : 0.00434882124055848 \t\n",
      "Vocab  16 - are : 0.011215381094071872 \t\n",
      "Vocab  17 - seeing : 0.00434882124055848 \t\n",
      "Vocab  18 - green : 0.00434882124055848 \t\n",
      "Vocab  19 - again. : 0.00434882124055848 \t\n"
     ]
    }
   ],
   "source": [
    "# The class implements topic modeling (Latent dirichlet allocation) algorithm using collapsed gibbs sampling as in inference. \n",
    "class LDA:\n",
    "    # topics to extract from the data (Components)\n",
    "    _numTopics = None\n",
    "    # vocabulary (unique words) in the dataset\n",
    "    _arrVocab = None\n",
    "    #size of vocabulary (count of unique words)\n",
    "    _numVocabSize = None\n",
    "    # dataset\n",
    "    _arrDocs = []\n",
    "    # dataset size (number of documents)\n",
    "    _numDocSize = None\n",
    "    # dirichlet prior (document to topic prior)\n",
    "    _numAlpha = None\n",
    "    # dirichlet prior (topic to word prior)\n",
    "    _numBeta = None\n",
    "    _ifScalarHyperParameters = True\n",
    "    # Gibb sampler iterations\n",
    "    _numGSIterations = None\n",
    "    # The iterations for initial burnin (update of parameters)\n",
    "    _numNBurnin = None\n",
    "    # The iterations for continuous burnin (update of parameters)\n",
    "    _numSampleLag = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # The following attributes are for internal working\n",
    "    __numTAlpha = None  \n",
    "    __numVBeta = None   \n",
    "    __arrTheta = None\n",
    "    __arrThetaSum = None\n",
    "    __arrPhi = None\n",
    "    __arrPhiSum = None\n",
    "    __arrNDT = None\n",
    "    __arrNDSum = []\n",
    "    __arrNTW = None\n",
    "    __arrNTSum = []\n",
    "    __arrZ = []\n",
    "    \n",
    "    # for alpha to be a list, its size must be equal to the size of the dataset, has value for each doc\n",
    "    # for beta to be a list, its size must be equal to the number of topics, has value for each topic  \n",
    "    def __init__(self, numTopics = 2, numAlpha = 1.0, numBeta = 0.1, \n",
    "                 numGSIterations = 2000, numNBurnin = 50, numSampleLag = 20):\n",
    "        self._numTopics = numTopics\n",
    "        self._numAlpha = numAlpha\n",
    "        self._numBeta = numBeta\n",
    "        self._numGSIterations = numGSIterations\n",
    "        self._numNBurni = numNBurnin\n",
    "        self._numSampleLag = numSampleLag\n",
    "            \n",
    "    #load data as integer encoding of words in a sequence (no padding or truncation)\n",
    "    def getData(self, path):\n",
    "        file = open(path, 'r')\n",
    "        rawData = file.read()\n",
    "        file.close()\n",
    "        self.__loadData(rawData)\n",
    "        self.__loadVocab()\n",
    "        self.__prepareCollections()\n",
    "\n",
    "    #load docs and docSize from the dataset\n",
    "    def __loadData(self, rawData):\n",
    "        rows = rawData.split('\\n')\n",
    "         \n",
    "        #read dataset as documents of words IDs\n",
    "        for row in rows:\n",
    "            swordlist = row.split('\\t')\n",
    "            swordlist = list(filter(None, swordlist))   #remove empty items from list\n",
    "            if len(swordlist) > 0:\n",
    "                iwordlist = [eval(w) for w in swordlist]    \n",
    "                self._arrDocs.append(iwordlist)\n",
    "\n",
    "        # determine dataset size\n",
    "        self._numDocSize = len(self._arrDocs)\n",
    "        \n",
    "        \n",
    "    #Determine unique words (vocabulary) and count of unique words (vocabSize)    \n",
    "    def __loadVocab(self):\n",
    "        #determine unique vocabulary\n",
    "        uniqueWords = []\n",
    "        for doc in self._arrDocs:\n",
    "            for word in doc:\n",
    "                if word not in uniqueWords:\n",
    "                    uniqueWords.append(word)\n",
    "        self._arrVocab = uniqueWords\n",
    "        self._numVocabSize = len(self._arrVocab)    \n",
    "\n",
    "    def __prepareCollections(self):\n",
    "        self.__arrNDSum = np.array([0] * self._numDocSize)\n",
    "        self.__arrTheta = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrThetasum = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        self.__arrNDT = np.array([[0] * self._numTopics] * self._numDocSize)\n",
    "        \n",
    "        self.__arrNTSum = np.array([0] * self._numTopics)\n",
    "        self.__arrPhi = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrPhisum = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "        self.__arrNTW = np.array([[0] * self._numVocabSize] * self._numTopics)\n",
    "\n",
    "        #Assign values to parameters based on hyper-parameters\n",
    "        self.__numTAlpha = self._numTopics*self._numAlpha  \n",
    "        self.__numVBeta = self._numVocabSize*self._numBeta   \n",
    "\n",
    "        \n",
    "        for d in range(0, self._numDocSize):\n",
    "            rowOfZeros = [0] * len(self._arrDocs[d])\n",
    "            self.__arrZ.append(rowOfZeros)\n",
    "                \n",
    "    # Initialize first markov chain randomly\n",
    "    def randomMarkovChainInitialization(self):\n",
    "        \n",
    "        for d in range(self._numDocSize):\n",
    "            wta = []                        #wta - word topic assignment\n",
    "            doc = self._arrDocs[d]\n",
    "            for ind in range(len(doc)): \n",
    "                randtopic = random.randint(0, self._numTopics - 1)      # generate a topic number at random\n",
    "                self.__arrZ[d][ind] = randtopic\n",
    "                self.__arrNDT[d][randtopic] += 1\n",
    "                self.__arrNDSum[d] += 1\n",
    "                wordid = self._arrDocs[d][ind]\n",
    "                self.__arrNTW[randtopic][wordid] += 1\n",
    "                self.__arrNTSum[randtopic] += 1\n",
    "            \n",
    "    \n",
    "    #Inference (Collapsed Gibbs Sampling)\n",
    "    def gibbsSampling(self):\n",
    "        tAlpha = self._numAlpha * self._numTopics\n",
    "        vBeta = self._numBeta * self._numVocabSize            \n",
    "                    \n",
    "        for it in range(self._numGSIterations):\n",
    "            for d in range(self._numDocSize):\n",
    "                dsize = len(self._arrDocs[d])\n",
    "                for ind in range(dsize):\n",
    "                    # remove old topic from a word instance\n",
    "                    oldTopic = self.__arrZ[d][ind]\n",
    "                    wordid = self._arrDocs[d][ind]\n",
    "                    self.__arrNDT[d][oldTopic] -= 1\n",
    "                    self.__arrNDSum[d] -= 1\n",
    "                    self.__arrNTW[oldTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[oldTopic] -= 1   \n",
    "\n",
    "                    # find a new more appropriate tpoic for the word instanc as per current state of the model\n",
    "                    prob = [0] * self._numTopics\n",
    "                    \n",
    "                    for t in range(self._numTopics):\n",
    "                        prob[t] = ((self.__arrNDT[d][t] + self._numAlpha) / (self.__arrNDSum[d] + tAlpha)) * \\\n",
    "                            (self.__arrNTW[t][wordid] + self._numBeta) / (self.__arrNTSum[t] + vBeta)\n",
    "                    \n",
    "                    #cumulate multinomial\n",
    "                    cdf = prob\n",
    "                    for x in range(1, len(cdf)):\n",
    "                        cdf[x] += cdf[x-1]\n",
    "                    \n",
    "                    cutoff = random.random() * cdf[-1]\n",
    "                    newTopic = 0\n",
    "                    for i in range(len(cdf)):\n",
    "                        if cdf[i] > cutoff:\n",
    "                            newTopic = i\n",
    "                            \n",
    "                            \n",
    "                    #update as per new topic\n",
    "                    self.__arrNDT[d][newTopic] += 1\n",
    "                    self.__arrNDSum[d] += 1\n",
    "                    self.__arrNTW[newTopic][wordid] -= 1\n",
    "                    self.__arrNTSum[newTopic] -= 1\n",
    "                \n",
    "    def getTopicsPerDocument(self):\n",
    "        print(\"\\n***Topics per Document***\")\n",
    "        for d in range(self._numDocSize):\n",
    "            print(\"Document \", d, \":\")\n",
    "            for t in range(self._numTopics):\n",
    "                val = (self.__arrNDT[d][t]+self._numAlpha)/(self.__arrNDSum[d]+self.__numTAlpha)\n",
    "                print(\"Topic \", t, \":\", val, '\\t')\n",
    "                \n",
    "                    \n",
    "   \n",
    "    def getWordsPerTopic(self, revdictionary):\n",
    "        print(\"\\n***Words per Topic***\")\n",
    "        \n",
    "        for t in range(self._numTopics):\n",
    "            print(\"Topic \", t, \":\")\n",
    "            i = 0\n",
    "            for v in range(self._numVocabSize):\n",
    "                val = (self.__arrNTW[t][v]+self._numBeta)/(self.__arrNTSum[t]+self.__numVBeta)\n",
    "                print(\"Vocab \", v, '-', revdictionary[str(v)], \":\", val, '\\t')\n",
    "                i = i + 1\n",
    "                if i == 20:\n",
    "                    break\n",
    "            \n",
    "    \n",
    "    def printall(self):\n",
    "        print(\"topics: \", self._numTopics)\n",
    "        print(\"dataset: \", self._arrDocs)\n",
    "        print(\"dataset size: \", self._numDocSize)\n",
    "        print(\"vocab: \", self._arrVocab)\n",
    "        print(\"vocab size: \", self._numVocabSize)\n",
    "        print(\"ndt: \", self.__arrNDT)\n",
    "        print(\"ndsum: \", self.__arrNDSum)\n",
    "        print(\"ntw: \", self.__arrNTW)\n",
    "        print(\"ntsum: \", self.__arrNTSum)\n",
    "        print(\"z: \", self.__arrZ)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    lda = LDA(numGSIterations = 1)\n",
    "    lda.getData(\"data/integer-encoded-data.txt\")\n",
    "    lda.randomMarkovChainInitialization()\n",
    "    lda.gibbsSampling()\n",
    "    lda.getTopicsPerDocument()\n",
    "    with open('data/revdictionary.json', 'r') as file:\n",
    "            revdictionary = json.load(file)\n",
    "    lda.getWordsPerTopic(revdictionary)\n",
    "    #lda.printall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01a9dc5-8d49-4f12-901e-feba48947036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49454d2b-c9a3-4003-9f40-17dddbf36b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fbbe7-3fc8-4d63-afce-c486f835b679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
